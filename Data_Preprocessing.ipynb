{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ic1u-rhZ96-JPSFQD_mo6bb3s-tLRfaR",
      "authorship_tag": "ABX9TyOABJx6PQ5JKWOy5C/viRJU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanaya2012/QA-chatbot/blob/main/Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-processing to get the relevant information from the given pdf"
      ],
      "metadata": {
        "id": "5SwbZ3W0M5go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates the process of extracting text from a PDF document containing two columns of data and filtering out unnecessary information. By implementing techniques such as parsing and data manipulation, the notebook showcases how to extract the desired text for further analysis or processing."
      ],
      "metadata": {
        "id": "TtjMq6svI10_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries\n"
      ],
      "metadata": {
        "id": "A-aNH152JHgo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EUkBiHRBsOS",
        "outputId": "e9218fdf-907e-4469-cc20-91d51c762183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.22.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "\n",
        "import string\n",
        "import spacy\n",
        "import re"
      ],
      "metadata": {
        "id": "mh2dspGgBxuk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_paths = ['/content/drive/MyDrive/mm6411.pdf', '/content/drive/MyDrive/mm6549a5.pdf', '/content/drive/MyDrive/mm6832a3-H.pdf', '/content/drive/MyDrive/rr6305.pdf']"
      ],
      "metadata": {
        "id": "PqtI6MVlB1la"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to get the most used font from the document\n"
      ],
      "metadata": {
        "id": "u32KkySPJKIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"fonts\" function extracts fonts and their usage in PDF documents. It takes a list of PDF document paths as input. The function iterates through each document, reads the text blocks, lines, and spans, and records the frequency of each font and size combination. Finally, it returns the most frequently used font and size as a tuple (text_font, text_style)."
      ],
      "metadata": {
        "id": "BRlCCiDuJ2Ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fonts(paths):\n",
        "    \"\"\"Extracts fonts and their usage in PDF documents.\n",
        "    :param paths: link to pdf documents\n",
        "    :type doc: list\n",
        "    :rtype: (text_font, text_style)\n",
        "    :return: most used font and size in the whole text\n",
        "    \"\"\"\n",
        "    styles = {}\n",
        "    font_counts = {}\n",
        "\n",
        "    for path in pdf_paths:\n",
        "      with fitz.open(path) as doc:\n",
        "        for page in doc:\n",
        "          blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "          for b in blocks:  # iterate through the text blocks\n",
        "              if b['type'] == 0:  # block contains text\n",
        "                  for l in b[\"lines\"]:  # iterate through the text lines\n",
        "                      for s in l[\"spans\"]:  # iterate through the text spans\n",
        "                        identifier= (s['size'], s['font'])\n",
        "                        if identifier in font_counts.keys():\n",
        "                          font_counts[identifier] += 1\n",
        "                        else:\n",
        "                          font_counts[identifier] = 1\n",
        "\n",
        "    keys = list(font_counts.keys())\n",
        "    values = list(font_counts.values())\n",
        "    sorted_value_index = np.argsort(values)[::-1]\n",
        "    font_counts = {keys[i]: values[i] for i in sorted_value_index}\n",
        "\n",
        "    p_style = list(font_counts.keys())[0]\n",
        "\n",
        "    return p_style"
      ],
      "metadata": {
        "id": "c7jRzwCBB_ES"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to get the text from the paragraphs having the most common font and size"
      ],
      "metadata": {
        "id": "Fbhw4O73JQ8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"get_para\" function retrieves paragraphs from PDF documents that match a specific font style given by \"p_style\". It takes a list of PDF document paths and the target font style as input. The function iterates through each document and page, extracting text blocks and lines. It analyzes the font style of each span and records the most frequently used font and size combination within each block. If the most used style matches the target font style, the text from that block is added to a list of header and paragraph combinations. Finally, the function returns the list of header and paragraph strings."
      ],
      "metadata": {
        "id": "_8H0FQsqKHCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_para(paths, p_style):\n",
        "   \n",
        "    header_para = []  # list with headers and paragraphs\n",
        "    first = True  # boolean operator for first header\n",
        "    previous_s = {}  # previous span\n",
        "\n",
        "    for path in pdf_paths:\n",
        "      with fitz.open(path) as doc:\n",
        "        for page in doc:\n",
        "          blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "          for b in blocks:\n",
        "            style_dict = {}\n",
        "            block_string = \"\"  # text found in block\n",
        "            # iterate through the text blocks\n",
        "            if b['type'] == 0:  # this block contains text\n",
        "                for l in b[\"lines\"]:  # iterate through the text lines\n",
        "                    for s in l[\"spans\"]:\n",
        "                      if s['text'].strip():  # iterate through the text spans\n",
        "                        font_size = s['size']\n",
        "                        span_font = s['font']\n",
        "                        \n",
        "                        key = (span_font, font_size)\n",
        "                        if key in style_dict.keys():\n",
        "                          style_dict[key] += 1\n",
        "                        else:\n",
        "                          style_dict[key] = 1\n",
        "\n",
        "                most_used_style = max(zip(style_dict.values(), style_dict.keys()))[1]\n",
        "                if most_used_style[0] == p_style[1] and most_used_style[1] == p_style[0]:\n",
        "                  for l in b[\"lines\"]:  # iterate through the text lines\n",
        "                    for s in l[\"spans\"]:\n",
        "                      block_string += s['text']\n",
        "                header_para.append(block_string)\n",
        "\n",
        "    return header_para"
      ],
      "metadata": {
        "id": "B2061VKACCIJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fomatting the string such that it has no punctuations and unnecessary numbers"
      ],
      "metadata": {
        "id": "Bud0GjEkJdD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_style = fonts(pdf_paths)\n",
        "pdf_text = get_para(pdf_paths, p_style)\n",
        "pdf_text = list(filter(None, pdf_text))\n",
        "pdf_text = ' '.join(pdf_text)\n",
        "pdf_text = re.sub(r'http\\S+', '', pdf_text, flags=re.MULTILINE)"
      ],
      "metadata": {
        "id": "-PuJ7zq4CG6x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = re.split('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', pdf_text)\n",
        "punctuations = string.punctuation\n",
        "tokens = [test_str.translate(str.maketrans('', '', string.punctuation)) for test_str in tokens]"
      ],
      "metadata": {
        "id": "66zlGQ_jCXMh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, token in enumerate(tokens):\n",
        "  words = token.split(\" \")\n",
        "  last_word = words[-1]\n",
        "  if last_word.isdigit():\n",
        "    tokens[idx] = ' '.join(token.split(\" \")[:-1])"
      ],
      "metadata": {
        "id": "Sz6q0r1eCZnp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "def clean_sentence(sentence, stopwords=False):\n",
        "  sentence = sentence.lower().strip()\n",
        "  sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
        "  if stopwords:\n",
        "    sentence = remove_stopwords(sentence)\n",
        "  return sentence\n",
        "\n",
        "def get_cleaned_sentences(tokens, stopwords=False):\n",
        "  cleaned_sentences = []\n",
        "  for row in tokens:\n",
        "    cleaned = clean_sentence(row, stopwords)\n",
        "    cleaned_sentences.append(cleaned)\n",
        "  return cleaned_sentences"
      ],
      "metadata": {
        "id": "d1zW4xsoCbJy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentences = get_cleaned_sentences(tokens, stopwords=True)\n",
        "cleaned_sentences_with_stopwords = get_cleaned_sentences(tokens, stopwords=False)\n",
        "pdf_text = ' '.join(cleaned_sentences_with_stopwords)"
      ],
      "metadata": {
        "id": "Fe7nkL_tCcxk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save back into the directory"
      ],
      "metadata": {
        "id": "AO6-REoaJm7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/pdf_text.txt\", \"a\")\n",
        "a = file.write(pdf_text)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "RndKW1YrCsDg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(r\"/content/drive/MyDrive/cleaned_sentences.txt\", 'w') as fp:\n",
        "    for item in cleaned_sentences_with_stopwords:\n",
        "        # write each item on a new line\n",
        "        fp.write(\"%s\\n\" % item)\n",
        "    print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtruSJT6DFOg",
        "outputId": "37e8bca1-bd8a-4f3c-bff6-66fda3f47b26"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kVQrat9WDosQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}